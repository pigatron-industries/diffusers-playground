{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rob/workspace/projects/diffusers-playground/notebooks/test_interrogator.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rob/workspace/projects/diffusers-playground/notebooks/test_interrogator.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mclip_interrogator\u001b[39;00m \u001b[39mimport\u001b[39;00m Interrogator, Config\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rob/workspace/projects/diffusers-playground/notebooks/test_interrogator.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39m/Users/rob/GoogleDrive/stable-diffusion/2022-12-batches/txt2img_1671462991.png\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rob/workspace/projects/diffusers-playground/notebooks/test_interrogator.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ci \u001b[39m=\u001b[39m Interrogator(Config(clip_model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mViT-L-14/openai\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rob/workspace/projects/diffusers-playground/notebooks/test_interrogator.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(ci\u001b[39m.\u001b[39minterrogate(image))\n",
      "File \u001b[0;32m~/workspace/projects/diffusers-playground/workspace/src/clip-interrogator/clip_interrogator/clip_interrogator.py:58\u001b[0m, in \u001b[0;36mInterrogator.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     56\u001b[0m configs_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(blip_path), \u001b[39m'\u001b[39m\u001b[39mconfigs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m med_config \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(configs_path, \u001b[39m'\u001b[39m\u001b[39mmed_config.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m blip_model \u001b[39m=\u001b[39m blip_decoder(\n\u001b[1;32m     59\u001b[0m     pretrained\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mblip_model_url, \n\u001b[1;32m     60\u001b[0m     image_size\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mblip_image_eval_size, \n\u001b[1;32m     61\u001b[0m     vit\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlarge\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     62\u001b[0m     med_config\u001b[39m=\u001b[39;49mmed_config\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m blip_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     65\u001b[0m blip_model \u001b[39m=\u001b[39m blip_model\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/workspace/projects/diffusers-playground/workspace/./src/blip/models/blip.py:173\u001b[0m, in \u001b[0;36mblip_decoder\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mblip_decoder\u001b[39m(pretrained\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 173\u001b[0m     model \u001b[39m=\u001b[39m BLIP_Decoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m pretrained:\n\u001b[1;32m    175\u001b[0m         model,msg \u001b[39m=\u001b[39m load_checkpoint(model,pretrained)\n",
      "File \u001b[0;32m~/workspace/projects/diffusers-playground/workspace/./src/blip/models/blip.py:95\u001b[0m, in \u001b[0;36mBLIP_Decoder.__init__\u001b[0;34m(self, med_config, image_size, vit, vit_grad_ckpt, vit_ckpt_layer, prompt)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m    med_config (str): path for the mixture of encoder-decoder model's configuration file\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m    image_size (int): input image size\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m    vit (str): model size of vision transformer\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m\"\"\"\u001b[39;00m            \n\u001b[1;32m     93\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual_encoder, vision_width \u001b[39m=\u001b[39m create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m init_tokenizer()   \n\u001b[1;32m     97\u001b[0m med_config \u001b[39m=\u001b[39m BertConfig\u001b[39m.\u001b[39mfrom_json_file(med_config)\n",
      "File \u001b[0;32m~/workspace/projects/diffusers-playground/workspace/./src/blip/models/blip.py:205\u001b[0m, in \u001b[0;36mcreate_vit\u001b[0;34m(vit, image_size, use_grad_checkpointing, ckpt_layer, drop_path_rate)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39melif\u001b[39;00m vit\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlarge\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    204\u001b[0m     vision_width \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m\n\u001b[0;32m--> 205\u001b[0m     visual_encoder \u001b[39m=\u001b[39m VisionTransformer(img_size\u001b[39m=\u001b[39;49mimage_size, patch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, embed_dim\u001b[39m=\u001b[39;49mvision_width, depth\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m, \n\u001b[1;32m    206\u001b[0m                                        num_heads\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, use_grad_checkpointing\u001b[39m=\u001b[39;49muse_grad_checkpointing, ckpt_layer\u001b[39m=\u001b[39;49mckpt_layer,\n\u001b[1;32m    207\u001b[0m                                        drop_path_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m \u001b[39mor\u001b[39;49;00m drop_path_rate\n\u001b[1;32m    208\u001b[0m                                       )   \n\u001b[1;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m visual_encoder, vision_width\n",
      "File \u001b[0;32m~/workspace/projects/diffusers-playground/workspace/./src/blip/models/vit.py:165\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, num_classes, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_scale, representation_size, drop_rate, attn_drop_rate, drop_path_rate, norm_layer, use_grad_checkpointing, ckpt_layer)\u001b[0m\n\u001b[1;32m    163\u001b[0m trunc_normal_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed, std\u001b[39m=\u001b[39m\u001b[39m.02\u001b[39m)\n\u001b[1;32m    164\u001b[0m trunc_normal_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token, std\u001b[39m=\u001b[39m\u001b[39m.02\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_weights)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:728\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    725\u001b[0m \n\u001b[1;32m    726\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 728\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    729\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    730\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:728\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    725\u001b[0m \n\u001b[1;32m    726\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 728\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    729\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    730\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.apply at line 728 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:728\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    725\u001b[0m \n\u001b[1;32m    726\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 728\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    729\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    730\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:729\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m    728\u001b[0m     module\u001b[39m.\u001b[39mapply(fn)\n\u001b[0;32m--> 729\u001b[0m fn(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    730\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/workspace/projects/diffusers-playground/workspace/./src/blip/models/vit.py:169\u001b[0m, in \u001b[0;36mVisionTransformer._init_weights\u001b[0;34m(self, m)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init_weights\u001b[39m(\u001b[39mself\u001b[39m, m):\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(m, nn\u001b[39m.\u001b[39mLinear):\n\u001b[0;32m--> 169\u001b[0m         trunc_normal_(m\u001b[39m.\u001b[39;49mweight, std\u001b[39m=\u001b[39;49m\u001b[39m.02\u001b[39;49m)\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(m, nn\u001b[39m.\u001b[39mLinear) \u001b[39mand\u001b[39;00m m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m             nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mconstant_(m\u001b[39m.\u001b[39mbias, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/timm/models/layers/weight_init.py:67\u001b[0m, in \u001b[0;36mtrunc_normal_\u001b[0;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Fills the input Tensor with values drawn from a truncated\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mnormal distribution. The values are effectively drawn from the\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mnormal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m    >>> nn.init.trunc_normal_(w)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m _trunc_normal_(tensor, mean, std, a, b)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/timm/models/layers/weight_init.py:28\u001b[0m, in \u001b[0;36m_trunc_normal_\u001b[0;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[1;32m     24\u001b[0m u \u001b[39m=\u001b[39m norm_cdf((b \u001b[39m-\u001b[39m mean) \u001b[39m/\u001b[39m std)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Uniformly fill tensor with values from [l, u], then translate to\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# [2l-1, 2u-1].\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m l \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m u \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Use inverse cdf transform for normal distribution to get truncated\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# standard normal\u001b[39;00m\n\u001b[1;32m     32\u001b[0m tensor\u001b[39m.\u001b[39merfinv_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import path\n",
    "path.setPathLocalNotebook()\n",
    "\n",
    "from PIL import Image\n",
    "from clip_interrogator import Interrogator, Config\n",
    "\n",
    "image = Image.open(\"/Users/rob/GoogleDrive/stable-diffusion/2022-12-batches/txt2img_1671462991.png\").convert('RGB')\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))\n",
    "print(ci.interrogate(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
